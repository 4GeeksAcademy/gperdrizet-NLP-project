{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network: spam detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading\n",
    "\n",
    "### 1.1. Load the data from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file into dataframe\n",
    "data_df = pd.read_csv('https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv')\n",
    "\n",
    "# Drop duplicates if any\n",
    "data_df.drop_duplicates(inplace=True)\n",
    "data_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Save a local copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory for raw data\n",
    "Path('../data/raw').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save a local copy of the raw data\n",
    "data_df.to_parquet('../data/raw/urls.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA\n",
    "\n",
    "### 2.1. Label frequency\n",
    "\n",
    "First, let's just take a look at how many 'spam' vs 'not spam' urls we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = data_df['is_spam'].value_counts()\n",
    "\n",
    "not_spam = label_counts.iloc[0]\n",
    "spam = label_counts.iloc[1]\n",
    "\n",
    "print(f'URLs are {(not_spam/(spam + not_spam)*100):.1f}% not spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is unbalanced, but not extremely so - we may not need to do anything with this information. But, it is good to keep it in mind as we work through EDA and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. URL length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['URL_length'] = data_df['url'].str.len().tolist()\n",
    "\n",
    "plt.title('URL length distribution')\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('URLs')\n",
    "plt.hist(data_df['URL_length'], bins=30, color='black')\n",
    "plt.show()\n",
    "\n",
    "print(f\"URL length mean: {np.mean(data_df['URL_length']):.0f}\")\n",
    "print(f\"URL length min: {min(data_df['URL_length']):.0f}\")\n",
    "print(f\"URL length max: {max(data_df['URL_length']):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Short URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_urls = data_df[data_df['URL_length'] < 20]\n",
    "short_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Long URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_urls = data_df[data_df['URL_length'] > 200]\n",
    "long_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['is_spam'] = data_df['is_spam'].astype(str)\n",
    "data_df['is_spam'] = data_df['is_spam'].replace({'True': '1', 'False': '0'})\n",
    "data_df['is_spam'] = data_df['is_spam'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. URL splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_splitter(url:str) -> list:\n",
    "    '''Splits URLs on non-word characters, then joins on space\n",
    "    for compatibility with the Tensorflow text vectorizer'''\n",
    "\n",
    "    return ' '.join(re.findall(f'[\\w`]+', url))\n",
    "\n",
    "data_df['url'] = data_df['url'].apply(domain_splitter)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data_df, test_size=0.3, random_state=RANDOM_SEED)\n",
    "print(f'Test data: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df = train_test_split(train_df, test_size=0.3, random_state=RANDOM_SEED)\n",
    "print(f'Training data: {train_df.shape}')\n",
    "print(f'Validation data: {validation_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RNN model\n",
    "\n",
    "### 3.2. Text encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_splitter(url:str) -> list:\n",
    "    '''Splits URLs on non-word characters'''\n",
    "\n",
    "    return re.findall(f'[\\w`]+', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = tf.convert_to_tensor(train_df['url'].to_numpy())\n",
    "validation_features = tf.convert_to_tensor(validation_df['url'].to_numpy())\n",
    "testing_features = tf.convert_to_tensor(test_df['url'].to_numpy())\n",
    "\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    ngrams=3,\n",
    "    output_mode='tf_idf',\n",
    "    sparse=False,\n",
    "    split='whitespace'\n",
    ")\n",
    "\n",
    "encoder.adapt(training_features)\n",
    "\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True\n",
    "    ),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(train_df['is_spam'])\n",
    "pos = sum(train_df['is_spam'])\n",
    "neg = total-pos\n",
    "\n",
    "weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "training_results = model.fit(\n",
    "    training_features,\n",
    "    train_df['is_spam'],\n",
    "    epochs=200,\n",
    "    validation_data=(validation_features, validation_df['is_spam']),\n",
    "    class_weight=class_weight,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Set-up a 1x2 figure for accuracy and binary cross-entropy\n",
    "fig, axs = plt.subplots(1,2, figsize=(8,4))\n",
    "\n",
    "# Add the main title\n",
    "fig.suptitle('RNN training curves', size='large')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "axs[0].set_title('Accuracy')\n",
    "axs[0].plot(np.array(training_results.history['accuracy']) * 100, label='Training')\n",
    "axs[0].plot(np.array(training_results.history['val_accuracy']) * 100, label='Validation')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Accuracy (%)')\n",
    "axs[0].legend(loc='best')\n",
    "\n",
    "# Plot training and validation binary cross-entropy\n",
    "axs[1].set_title('Binary cross-entropy')\n",
    "axs[1].plot(training_results.history['loss'])\n",
    "axs[1].plot(training_results.history['val_loss'])\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Binary cross-entropy')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Add regularization to the model\n",
    "regularizer=tf.keras.regularizers.L1L2(l1=0.02, l2=0.002)\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=128,\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizer),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Re-compile\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Re-train\n",
    "training_results=model.fit(\n",
    "    training_features,\n",
    "    training_labels,\n",
    "    epochs=500,\n",
    "    validation_data=(validation_features,validation_labels),\n",
    "    class_weight=class_weight,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Set-up a 1x2 figure for accuracy and binary cross-entropy\n",
    "fig, axs=plt.subplots(1,2, figsize=(8,4))\n",
    "\n",
    "# Add the main title\n",
    "fig.suptitle('RNN training curves', size='large')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "axs[0].set_title('Accuracy')\n",
    "axs[0].plot(np.array(training_results.history['accuracy']) * 100, label='Training')\n",
    "axs[0].plot(np.array(training_results.history['val_accuracy']) * 100, label='Validation')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Accuracy (%)')\n",
    "axs[0].legend(loc='best')\n",
    "\n",
    "# Plot training and validation binary cross-entropy\n",
    "axs[1].set_title('Binary cross-entropy')\n",
    "axs[1].plot(training_results.history['loss'])\n",
    "axs[1].plot(training_results.history['val_loss'])\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Binary cross-entropy')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model evaluation\n",
    "\n",
    "### 4.1. Test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "threshold=0.5\n",
    "predictions=model.predict(tf.convert_to_tensor(testing_tokens))\n",
    "predictions=[1 if p > threshold else 0 for p in predictions]\n",
    "\n",
    "accuracy=accuracy_score(predictions, encoded_testing_labels)*100\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm=confusion_matrix(encoded_testing_labels, predictions, normalize='true')\n",
    "cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "_=cm_disp.plot()\n",
    "\n",
    "plt.title(f'Test set performance\\noverall accuracy: {accuracy:.1f}%')\n",
    "plt.xlabel('Predicted outcome')\n",
    "plt.ylabel('True outcome')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
